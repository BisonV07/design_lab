{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN2CQW5VKfgxslHocLJaPah",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BisonV07/Airline-Management/blob/main/DESIGN_LAB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: do you see the files uploaded\n",
        "\n",
        "!ls -l\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAM9qIGrL8SO",
        "outputId": "1ae787d6-8e5a-4688-b8e3-f0630eeab6db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 19532\n",
            "-rw-r--r-- 1 root root  1009963 Mar 20 14:13 03_26_trip1_sensors.csv\n",
            "-rw-r--r-- 1 root root   229022 Mar 20 14:13 03_26_trip2_sensors.csv\n",
            "-rw-r--r-- 1 root root  1123031 Mar 20 14:13 04_02_trip1_sensors.csv\n",
            "-rw-r--r-- 1 root root   200651 Mar 20 14:13 merged_without_gyro.csv\n",
            "-rw-r--r-- 1 root root  3629715 Mar 20 14:14 reduced_road_features_2percent.csv\n",
            "-rw-r--r-- 1 root root 11013950 Mar 20 14:15 road_features.csv\n",
            "drwxr-xr-x 1 root root     4096 Mar 17 13:32 sample_data\n",
            "-rw-r--r-- 1 root root  2360346 Mar 20 14:13 sensor_with_gyro.csv\n",
            "-rw-r--r-- 1 root root   199453 Mar 20 14:13 sensor_without_gyro.csv\n",
            "-rw-r--r-- 1 root root       68 Mar 20 14:13 trip1_02-22-17_potholes.csv\n",
            "-rw-r--r-- 1 root root    32677 Mar 20 14:13 trip1_02-22-17_sensors.csv\n",
            "-rw-r--r-- 1 root root      101 Mar 20 14:13 trip2_02-22-17_potholes.csv\n",
            "-rw-r--r-- 1 root root    76213 Mar 20 14:13 trip2_02-22-17_sensors.csv\n",
            "-rw-r--r-- 1 root root      140 Mar 20 14:13 trip3_02-22-17_potholes.csv\n",
            "-rw-r--r-- 1 root root    88901 Mar 20 14:13 trip3_02-22-17_sensors.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "\n",
        "# Define file names (Assuming files are in the same directory)\n",
        "sensor_files = [\n",
        "    \"03_26_trip1_sensors.csv\",\n",
        "    \"03_26_trip2_sensors.csv\",\n",
        "    \"04_02_trip1_sensors.csv\",\n",
        "    \"trip1_02-22-17_sensors.csv\",\n",
        "    \"trip2_02-22-17_sensors.csv\",\n",
        "    \"trip3_02-22-17_sensors.csv\",\n",
        "]\n",
        "\n",
        "pothole_files = [\n",
        "    \"trip1_02-22-17_potholes.csv\",\n",
        "    \"trip2_02-22-17_potholes.csv\",\n",
        "    \"trip3_02-22-17_potholes.csv\",\n",
        "]\n",
        "\n",
        "# Lists to store categorized data\n",
        "sensor_with_gyro = []\n",
        "sensor_without_gyro = []\n",
        "\n",
        "# Load sensor data and categorize based on gyroscope presence\n",
        "for file in sensor_files:\n",
        "    df = pd.read_csv(file, encoding=\"utf-8\", engine=\"python\")\n",
        "\n",
        "    if {\"gyroX\", \"gyroY\", \"gyroZ\"}.issubset(df.columns):\n",
        "        sensor_with_gyro.append(df)  # Add to gyro group\n",
        "    else:\n",
        "        sensor_without_gyro.append(df)  # Add to non-gyro group\n",
        "\n",
        "# Combine each group separately\n",
        "sensor_with_gyro_df = pd.concat(sensor_with_gyro, ignore_index=True) if sensor_with_gyro else pd.DataFrame()\n",
        "sensor_without_gyro_df = pd.concat(sensor_without_gyro, ignore_index=True) if sensor_without_gyro else pd.DataFrame()\n",
        "\n",
        "# Load pothole data\n",
        "pothole_dfs = [pd.read_csv(file, encoding=\"utf-8\", engine=\"python\") for file in pothole_files]\n",
        "pothole_df = pd.concat(pothole_dfs, ignore_index=True) if pothole_dfs else pd.DataFrame()\n",
        "\n",
        "# ‚úÖ Compute statistics (Mean & Variance) for each group\n",
        "if not sensor_with_gyro_df.empty:\n",
        "    gyro_stats = sensor_with_gyro_df[[\"gyroX\", \"gyroY\", \"gyroZ\"]].agg([\"mean\", \"var\"])\n",
        "    print(\"\\n‚úÖ Gyroscope Data Statistics:\")\n",
        "    print(gyro_stats)\n",
        "\n",
        "if not sensor_without_gyro_df.empty:\n",
        "    accel_stats = sensor_without_gyro_df[[\"accelerometerX\", \"accelerometerY\", \"accelerometerZ\"]].agg([\"mean\", \"var\"])\n",
        "    print(\"\\n‚úÖ Accelerometer Data Statistics:\")\n",
        "    print(accel_stats)\n",
        "\n",
        "# ‚úÖ Fix timestamp type for merging\n",
        "if not sensor_with_gyro_df.empty:\n",
        "    sensor_with_gyro_df[\"timestamp\"] = sensor_with_gyro_df[\"timestamp\"].astype(float)\n",
        "\n",
        "if not sensor_without_gyro_df.empty:\n",
        "    sensor_without_gyro_df[\"timestamp\"] = sensor_without_gyro_df[\"timestamp\"].astype(float)\n",
        "\n",
        "if not pothole_df.empty:\n",
        "    pothole_df[\"timestamp\"] = pothole_df[\"timestamp\"].astype(float)\n",
        "\n",
        "# ‚úÖ Merge pothole data separately for both groups\n",
        "merged_with_gyro = pd.merge(sensor_with_gyro_df, pothole_df, on=\"timestamp\", how=\"outer\") if not sensor_with_gyro_df.empty else pd.DataFrame()\n",
        "merged_without_gyro = pd.merge(sensor_without_gyro_df, pothole_df, on=\"timestamp\", how=\"outer\") if not sensor_without_gyro_df.empty else pd.DataFrame()\n",
        "\n",
        "# ‚úÖ Save outputs as CSV (Optional)\n",
        "sensor_with_gyro_df.to_csv(\"sensor_with_gyro.csv\", index=False)\n",
        "sensor_without_gyro_df.to_csv(\"sensor_without_gyro.csv\", index=False)\n",
        "merged_with_gyro.to_csv(\"merged_with_gyro.csv\", index=False)\n",
        "merged_without_gyro.to_csv(\"merged_without_gyro.csv\", index=False)\n",
        "\n",
        "# ‚úÖ Print success message\n",
        "print(\"\\nüöÄ Data successfully processed and saved!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70A1_RFeMe01",
        "outputId": "fee39d82-a42e-444c-dfbf-817f0234c80a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Gyroscope Data Statistics:\n",
            "         gyroX     gyroY     gyroZ\n",
            "mean -0.027476 -0.006836 -0.016614\n",
            "var   1.305936  1.320774  1.306125\n",
            "\n",
            "‚úÖ Accelerometer Data Statistics:\n",
            "      accelerometerX  accelerometerY  accelerometerZ\n",
            "mean        0.010357       -0.982846        0.170081\n",
            "var         0.016795        0.013505        0.016996\n",
            "\n",
            "üöÄ Data successfully processed and saved!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define filenames of processed CSVs\n",
        "filtered_files = [\n",
        "    \"sensor_with_gyro.csv\",\n",
        "    \"sensor_without_gyro.csv\",\n",
        "    \"merged_with_gyro.csv\",\n",
        "    \"merged_without_gyro.csv\",\n",
        "]\n",
        "\n",
        "# Print each dataset with proper formatting\n",
        "for file in filtered_files:\n",
        "    try:\n",
        "        df = pd.read_csv(file)  # Read the filtered dataset\n",
        "        print(f\"\\nüîπ {file} - First 5 Rows:\\n\")\n",
        "        print(df.head())  # Print first 5 rows\n",
        "        print(\"\\nüîπ Summary:\\n\")\n",
        "        print(df.info())  # Print column info\n",
        "        print(\"\\nüîπ Missing Values:\\n\")\n",
        "        print(df.isnull().sum())  # Print missing values count\n",
        "        print(\"=\" * 50)  # Separator for better readability\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ö†Ô∏è Error reading {file}: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dXCQxh55G-x",
        "outputId": "6fea518d-5fbf-4c91-9042-e6fb86895888"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîπ sensor_with_gyro.csv - First 5 Rows:\n",
            "\n",
            "      timestamp   latitude  longitude  speed  accelerometerX  accelerometerY  \\\n",
            "0  1.490542e+09  40.442152 -79.939125    0.0        0.069946       -0.955444   \n",
            "1  1.490542e+09  40.442152 -79.939125    0.0        0.075745       -0.953049   \n",
            "2  1.490542e+09  40.442152 -79.939125    0.0        0.075439       -0.953644   \n",
            "3  1.490542e+09  40.442152 -79.939125    0.0        0.073151       -0.950409   \n",
            "4  1.490542e+09  40.442152 -79.939125    0.0        0.052338       -0.962402   \n",
            "\n",
            "   accelerometerZ       gyroX       gyroY       gyroZ  \n",
            "0        0.280807 -100.000000 -100.000000 -100.000000  \n",
            "1        0.251450   -0.017045    0.009625   -0.002960  \n",
            "2        0.253555   -0.013865    0.006415   -0.004074  \n",
            "3        0.248245   -0.011730   -0.001023    0.000147  \n",
            "4        0.269684   -0.013839    0.013866   -0.004058  \n",
            "\n",
            "üîπ Summary:\n",
            "\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 15381 entries, 0 to 15380\n",
            "Data columns (total 10 columns):\n",
            " #   Column          Non-Null Count  Dtype  \n",
            "---  ------          --------------  -----  \n",
            " 0   timestamp       15381 non-null  float64\n",
            " 1   latitude        15381 non-null  float64\n",
            " 2   longitude       15381 non-null  float64\n",
            " 3   speed           15381 non-null  float64\n",
            " 4   accelerometerX  15381 non-null  float64\n",
            " 5   accelerometerY  15381 non-null  float64\n",
            " 6   accelerometerZ  15381 non-null  float64\n",
            " 7   gyroX           15381 non-null  float64\n",
            " 8   gyroY           15381 non-null  float64\n",
            " 9   gyroZ           15381 non-null  float64\n",
            "dtypes: float64(10)\n",
            "memory usage: 1.2 MB\n",
            "None\n",
            "\n",
            "üîπ Missing Values:\n",
            "\n",
            "timestamp         0\n",
            "latitude          0\n",
            "longitude         0\n",
            "speed             0\n",
            "accelerometerX    0\n",
            "accelerometerY    0\n",
            "accelerometerZ    0\n",
            "gyroX             0\n",
            "gyroY             0\n",
            "gyroZ             0\n",
            "dtype: int64\n",
            "==================================================\n",
            "\n",
            "üîπ sensor_without_gyro.csv - First 5 Rows:\n",
            "\n",
            "   timestamp   latitude  longitude  speed  accelerometerX  accelerometerY  \\\n",
            "0        1.0  40.441984 -79.938870   0.00        0.009293       -0.989380   \n",
            "1        2.0  40.441992 -79.938869   0.53       -0.014648       -1.000565   \n",
            "2        3.0  40.441992 -79.938869   0.00        0.008209       -0.979630   \n",
            "3        4.0  40.441992 -79.938869   0.00        0.006882       -0.997696   \n",
            "4        5.0  40.441992 -79.938869   0.00        0.007156       -1.000488   \n",
            "\n",
            "   accelerometerZ  \n",
            "0        0.074951  \n",
            "1        0.107620  \n",
            "2        0.116135  \n",
            "3        0.106323  \n",
            "4        0.108673  \n",
            "\n",
            "üîπ Summary:\n",
            "\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1825 entries, 0 to 1824\n",
            "Data columns (total 7 columns):\n",
            " #   Column          Non-Null Count  Dtype  \n",
            "---  ------          --------------  -----  \n",
            " 0   timestamp       1825 non-null   float64\n",
            " 1   latitude        1825 non-null   float64\n",
            " 2   longitude       1825 non-null   float64\n",
            " 3   speed           1825 non-null   float64\n",
            " 4   accelerometerX  1825 non-null   float64\n",
            " 5   accelerometerY  1825 non-null   float64\n",
            " 6   accelerometerZ  1825 non-null   float64\n",
            "dtypes: float64(7)\n",
            "memory usage: 99.9 KB\n",
            "None\n",
            "\n",
            "üîπ Missing Values:\n",
            "\n",
            "timestamp         0\n",
            "latitude          0\n",
            "longitude         0\n",
            "speed             0\n",
            "accelerometerX    0\n",
            "accelerometerY    0\n",
            "accelerometerZ    0\n",
            "dtype: int64\n",
            "==================================================\n",
            "\n",
            "üîπ merged_with_gyro.csv - First 5 Rows:\n",
            "\n",
            "   timestamp  latitude  longitude  speed  accelerometerX  accelerometerY  \\\n",
            "0       12.0       NaN        NaN    NaN             NaN             NaN   \n",
            "1       17.0       NaN        NaN    NaN             NaN             NaN   \n",
            "2       19.0       NaN        NaN    NaN             NaN             NaN   \n",
            "3       22.0       NaN        NaN    NaN             NaN             NaN   \n",
            "4       27.0       NaN        NaN    NaN             NaN             NaN   \n",
            "\n",
            "   accelerometerZ  gyroX  gyroY  gyroZ  \n",
            "0             NaN    NaN    NaN    NaN  \n",
            "1             NaN    NaN    NaN    NaN  \n",
            "2             NaN    NaN    NaN    NaN  \n",
            "3             NaN    NaN    NaN    NaN  \n",
            "4             NaN    NaN    NaN    NaN  \n",
            "\n",
            "üîπ Summary:\n",
            "\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 15439 entries, 0 to 15438\n",
            "Data columns (total 10 columns):\n",
            " #   Column          Non-Null Count  Dtype  \n",
            "---  ------          --------------  -----  \n",
            " 0   timestamp       15439 non-null  float64\n",
            " 1   latitude        15381 non-null  float64\n",
            " 2   longitude       15381 non-null  float64\n",
            " 3   speed           15381 non-null  float64\n",
            " 4   accelerometerX  15381 non-null  float64\n",
            " 5   accelerometerY  15381 non-null  float64\n",
            " 6   accelerometerZ  15381 non-null  float64\n",
            " 7   gyroX           15381 non-null  float64\n",
            " 8   gyroY           15381 non-null  float64\n",
            " 9   gyroZ           15381 non-null  float64\n",
            "dtypes: float64(10)\n",
            "memory usage: 1.2 MB\n",
            "None\n",
            "\n",
            "üîπ Missing Values:\n",
            "\n",
            "timestamp          0\n",
            "latitude          58\n",
            "longitude         58\n",
            "speed             58\n",
            "accelerometerX    58\n",
            "accelerometerY    58\n",
            "accelerometerZ    58\n",
            "gyroX             58\n",
            "gyroY             58\n",
            "gyroZ             58\n",
            "dtype: int64\n",
            "==================================================\n",
            "\n",
            "üîπ merged_without_gyro.csv - First 5 Rows:\n",
            "\n",
            "   timestamp   latitude  longitude  speed  accelerometerX  accelerometerY  \\\n",
            "0        1.0  40.441984 -79.938870   0.00        0.009293       -0.989380   \n",
            "1        1.0  40.440645 -79.960023  13.50        0.018982       -0.893524   \n",
            "2        1.0  40.428912 -79.982350   5.15        0.084610       -0.875275   \n",
            "3        2.0  40.441992 -79.938869   0.53       -0.014648       -1.000565   \n",
            "4        2.0  40.440552 -79.960150  14.49        0.103287       -1.053253   \n",
            "\n",
            "   accelerometerZ  \n",
            "0        0.074951  \n",
            "1        0.069473  \n",
            "2        0.201309  \n",
            "3        0.107620  \n",
            "4        0.153046  \n",
            "\n",
            "üîπ Summary:\n",
            "\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1836 entries, 0 to 1835\n",
            "Data columns (total 7 columns):\n",
            " #   Column          Non-Null Count  Dtype  \n",
            "---  ------          --------------  -----  \n",
            " 0   timestamp       1836 non-null   float64\n",
            " 1   latitude        1836 non-null   float64\n",
            " 2   longitude       1836 non-null   float64\n",
            " 3   speed           1836 non-null   float64\n",
            " 4   accelerometerX  1836 non-null   float64\n",
            " 5   accelerometerY  1836 non-null   float64\n",
            " 6   accelerometerZ  1836 non-null   float64\n",
            "dtypes: float64(7)\n",
            "memory usage: 100.5 KB\n",
            "None\n",
            "\n",
            "üîπ Missing Values:\n",
            "\n",
            "timestamp         0\n",
            "latitude          0\n",
            "longitude         0\n",
            "speed             0\n",
            "accelerometerX    0\n",
            "accelerometerY    0\n",
            "accelerometerZ    0\n",
            "dtype: int64\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scipy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzRg0b3Uhj-h",
        "outputId": "a8d8a2ee-572c-4223-aef5-639d99d9e6c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.14.1)\n",
            "Requirement already satisfied: numpy<2.3,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy) (2.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below code is generating features to classify as good road or bad road\n"
      ],
      "metadata": {
        "id": "ZqkB8Rdm7eZ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "from scipy.fft import fft, fftfreq\n",
        "\n",
        "# Load sensor data (ensure timestamp is sorted)\n",
        "df = pd.read_csv(\"sensor_with_gyro.csv\")  # Modify as needed\n",
        "df = df.sort_values(by=\"timestamp\")  # Ensure data is time-ordered\n",
        "\n",
        "# Define window size (assuming timestamp is in seconds)\n",
        "WINDOW_SIZE = 1  # Seconds\n",
        "FPS = 10  # Adjust based on your data sampling rate (e.g., 10Hz means 10 samples/sec)\n",
        "WINDOW_LENGTH = WINDOW_SIZE * FPS  # Number of data points in a 2-sec window\n",
        "\n",
        "# Initialize moving window queues\n",
        "gyroX_queue = deque(maxlen=WINDOW_LENGTH)\n",
        "gyroY_queue = deque(maxlen=WINDOW_LENGTH)\n",
        "gyroZ_queue = deque(maxlen=WINDOW_LENGTH)\n",
        "accelX_queue = deque(maxlen=WINDOW_LENGTH)\n",
        "accelY_queue = deque(maxlen=WINDOW_LENGTH)\n",
        "accelZ_queue = deque(maxlen=WINDOW_LENGTH)\n",
        "\n",
        "# Function to compute time-domain features\n",
        "def compute_time_features(data):\n",
        "    return {\n",
        "        \"mean\": np.mean(data),\n",
        "        \"variance\": np.var(data),\n",
        "        \"std_dev\": np.std(data),\n",
        "        \"max\": np.max(data),\n",
        "        \"min\": np.min(data)\n",
        "    }\n",
        "\n",
        "# Function to compute FFT features\n",
        "def compute_fft_features(data, sampling_rate=FPS):\n",
        "    if len(data) < WINDOW_LENGTH:\n",
        "        return {\"fft_peak_freq\": 0, \"fft_peak_amplitude\": 0}  # Handle small windows\n",
        "\n",
        "    # Compute FFT\n",
        "    yf = np.abs(fft(data))  # Compute absolute FFT amplitudes\n",
        "    xf = fftfreq(len(data), 1 / sampling_rate)  # Compute frequencies\n",
        "\n",
        "    # Extract dominant frequency and amplitude\n",
        "    peak_idx = np.argmax(yf[1:]) + 1  # Ignore DC component (index 0)\n",
        "    return {\n",
        "        \"fft_peak_freq\": xf[peak_idx],\n",
        "        \"fft_peak_amplitude\": yf[peak_idx]\n",
        "    }\n",
        "\n",
        "# Process data using a moving window\n",
        "feature_data = []\n",
        "\n",
        "for i in range(len(df)):\n",
        "    # Add new data to queues\n",
        "    gyroX_queue.append(df.iloc[i][\"gyroX\"])\n",
        "    gyroY_queue.append(df.iloc[i][\"gyroY\"])\n",
        "    gyroZ_queue.append(df.iloc[i][\"gyroZ\"])\n",
        "    accelX_queue.append(df.iloc[i][\"accelerometerX\"])\n",
        "    accelY_queue.append(df.iloc[i][\"accelerometerY\"])\n",
        "    accelZ_queue.append(df.iloc[i][\"accelerometerZ\"])\n",
        "\n",
        "    # Ensure window is full before computing features\n",
        "    if len(gyroX_queue) == WINDOW_LENGTH:\n",
        "        # Compute time-domain and FFT features\n",
        "        gyro_features = {\n",
        "            \"gyroX_\" + key: val for key, val in compute_time_features(list(gyroX_queue)).items()\n",
        "        }\n",
        "        gyro_features.update({\n",
        "            \"gyroY_\" + key: val for key, val in compute_time_features(list(gyroY_queue)).items()\n",
        "        })\n",
        "        gyro_features.update({\n",
        "            \"gyroZ_\" + key: val for key, val in compute_time_features(list(gyroZ_queue)).items()\n",
        "        })\n",
        "\n",
        "        accel_features = {\n",
        "            \"accelX_\" + key: val for key, val in compute_time_features(list(accelX_queue)).items()\n",
        "        }\n",
        "        accel_features.update({\n",
        "            \"accelY_\" + key: val for key, val in compute_time_features(list(accelY_queue)).items()\n",
        "        })\n",
        "        accel_features.update({\n",
        "            \"accelZ_\" + key: val for key, val in compute_time_features(list(accelZ_queue)).items()\n",
        "        })\n",
        "\n",
        "        # Compute FFT features\n",
        "        fft_features = {\n",
        "            \"gyroX_\" + key: val for key, val in compute_fft_features(list(gyroX_queue)).items()\n",
        "        }\n",
        "        fft_features.update({\n",
        "            \"gyroY_\" + key: val for key, val in compute_fft_features(list(gyroY_queue)).items()\n",
        "        })\n",
        "        fft_features.update({\n",
        "            \"gyroZ_\" + key: val for key, val in compute_fft_features(list(gyroZ_queue)).items()\n",
        "        })\n",
        "        fft_features.update({\n",
        "            \"accelX_\" + key: val for key, val in compute_fft_features(list(accelX_queue)).items()\n",
        "        })\n",
        "        fft_features.update({\n",
        "            \"accelY_\" + key: val for key, val in compute_fft_features(list(accelY_queue)).items()\n",
        "        })\n",
        "        fft_features.update({\n",
        "            \"accelZ_\" + key: val for key, val in compute_fft_features(list(accelZ_queue)).items()\n",
        "        })\n",
        "\n",
        "        # Combine features\n",
        "        feature_vector = {\n",
        "            \"timestamp\": df.iloc[i][\"timestamp\"],\n",
        "            **gyro_features,\n",
        "            **accel_features,\n",
        "            **fft_features\n",
        "        }\n",
        "\n",
        "        feature_data.append(feature_vector)\n",
        "\n",
        "# Convert extracted features into a DataFrame\n",
        "feature_df = pd.DataFrame(feature_data)\n",
        "\n",
        "# Save features for model training\n",
        "feature_df.to_csv(\"road_features.csv\", index=False)\n",
        "\n",
        "print(\"\\nüöÄ Feature extraction complete! Saved as 'road_features.csv'.\")\n",
        "print(feature_df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJEE-iNm7PE1",
        "outputId": "039cbba3-ebaf-431f-d7f9-f92e5e202a89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üöÄ Feature extraction complete! Saved as 'road_features.csv'.\n",
            "      timestamp  gyroX_mean  gyroX_variance  gyroX_std_dev  gyroX_max  \\\n",
            "0  1.490542e+09  -10.013428      899.731463      29.995524  -0.011730   \n",
            "1  1.490542e+09   -0.015134        0.000007       0.002687  -0.011730   \n",
            "2  1.490542e+09   -0.014601        0.000008       0.002782  -0.011719   \n",
            "3  1.490542e+09   -0.015132        0.000009       0.003081  -0.011719   \n",
            "4  1.490542e+09   -0.015032        0.000010       0.003204  -0.010723   \n",
            "\n",
            "    gyroX_min  gyroY_mean  gyroY_variance  gyroY_std_dev  gyroY_max  ...  \\\n",
            "0 -100.000000   -9.992199      900.156049      30.002601   0.013866  ...   \n",
            "1   -0.021313    0.008763        0.000015       0.003887   0.013866  ...   \n",
            "2   -0.021313    0.008442        0.000015       0.003935   0.013866  ...   \n",
            "3   -0.021313    0.008446        0.000015       0.003933   0.013866  ...   \n",
            "4   -0.021313    0.007804        0.000031       0.005597   0.013866  ...   \n",
            "\n",
            "   gyroY_fft_peak_freq  gyroY_fft_peak_amplitude  gyroZ_fft_peak_freq  \\\n",
            "0                  4.0                100.018194                  2.0   \n",
            "1                  4.0                  0.017009                  4.0   \n",
            "2                  4.0                  0.017620                  4.0   \n",
            "3                  4.0                  0.017646                  1.0   \n",
            "4                  4.0                  0.024060                  2.0   \n",
            "\n",
            "   gyroZ_fft_peak_amplitude  accelX_fft_peak_freq  accelX_fft_peak_amplitude  \\\n",
            "0                 99.999417                   1.0                   0.042157   \n",
            "1                  0.008614                   1.0                   0.049950   \n",
            "2                  0.009210                   1.0                   0.050807   \n",
            "3                  0.008984                   1.0                   0.050975   \n",
            "4                  0.006753                   1.0                   0.071377   \n",
            "\n",
            "   accelY_fft_peak_freq  accelY_fft_peak_amplitude  accelZ_fft_peak_freq  \\\n",
            "0                  -5.0                   0.031754                  -5.0   \n",
            "1                   1.0                   0.024204                   1.0   \n",
            "2                  -5.0                   0.025360                   1.0   \n",
            "3                   1.0                   0.024736                   1.0   \n",
            "4                   1.0                   0.036411                   1.0   \n",
            "\n",
            "   accelZ_fft_peak_amplitude  \n",
            "0                   0.062561  \n",
            "1                   0.054084  \n",
            "2                   0.056728  \n",
            "3                   0.054445  \n",
            "4                   0.076676  \n",
            "\n",
            "[5 rows x 43 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "next is I want to apply PCA to reduce features ( to eliminate features which contribute less than 2% variance )"
      ],
      "metadata": {
        "id": "GT2CZPaG8OOE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "feature_df = pd.read_csv(\"road_features.csv\")\n",
        "\n",
        "# Drop timestamp (since it's not a feature)\n",
        "if \"timestamp\" in feature_df.columns:\n",
        "    feature_df = feature_df.drop(columns=[\"timestamp\"])\n",
        "\n",
        "# Standardize the data (mean = 0, variance = 1)\n",
        "scaler = StandardScaler()\n",
        "scaled_features = scaler.fit_transform(feature_df)\n",
        "\n",
        "# Apply PCA\n",
        "pca = PCA()\n",
        "pca.fit(scaled_features)\n",
        "\n",
        "# Get explained variance ratio\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "\n",
        "# Print variance contribution of each component\n",
        "print(\"\\nüîπ PCA Explained Variance (% Contribution of Each Component):\")\n",
        "for i, var in enumerate(explained_variance):\n",
        "    print(f\"Component {i+1}: {var:.2%}\")\n",
        "\n",
        "# Select components that contribute at least 2% variance\n",
        "important_components = np.where(explained_variance >= 0.02)[0]  # Indices of important PCs\n",
        "num_important_components = len(important_components)\n",
        "\n",
        "print(f\"\\n‚úÖ Keeping {num_important_components} components out of {len(explained_variance)} (‚â•2% variance)\")\n",
        "\n",
        "# Transform the data using only important components\n",
        "pca = PCA(n_components=num_important_components)\n",
        "reduced_features = pca.fit_transform(scaled_features)\n",
        "\n",
        "# Convert reduced data back to DataFrame\n",
        "reduced_feature_df = pd.DataFrame(reduced_features, columns=[f\"PCA_Component_{i+1}\" for i in range(num_important_components)])\n",
        "\n",
        "# Save the reduced dataset\n",
        "reduced_feature_df.to_csv(\"reduced_road_features_2percent.csv\", index=False)\n",
        "\n",
        "# Print summary\n",
        "print(\"\\nüöÄ PCA completed! Reduced dataset saved as 'reduced_road_features_2percent.csv'.\")\n",
        "print(\"\\nüîπ First 5 Rows of Reduced Feature Set:\")\n",
        "print(reduced_feature_df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oElG9szf8XF5",
        "outputId": "148e7de3-0787-4a4c-85e1-ef6876b0a210"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîπ PCA Explained Variance (% Contribution of Each Component):\n",
            "Component 1: 35.52%\n",
            "Component 2: 26.43%\n",
            "Component 3: 6.45%\n",
            "Component 4: 4.63%\n",
            "Component 5: 3.18%\n",
            "Component 6: 2.96%\n",
            "Component 7: 2.62%\n",
            "Component 8: 2.53%\n",
            "Component 9: 2.35%\n",
            "Component 10: 2.27%\n",
            "Component 11: 2.24%\n",
            "Component 12: 2.08%\n",
            "Component 13: 1.66%\n",
            "Component 14: 1.21%\n",
            "Component 15: 1.02%\n",
            "Component 16: 0.83%\n",
            "Component 17: 0.61%\n",
            "Component 18: 0.31%\n",
            "Component 19: 0.21%\n",
            "Component 20: 0.20%\n",
            "Component 21: 0.15%\n",
            "Component 22: 0.12%\n",
            "Component 23: 0.10%\n",
            "Component 24: 0.09%\n",
            "Component 25: 0.09%\n",
            "Component 26: 0.05%\n",
            "Component 27: 0.02%\n",
            "Component 28: 0.02%\n",
            "Component 29: 0.01%\n",
            "Component 30: 0.01%\n",
            "Component 31: 0.01%\n",
            "Component 32: 0.00%\n",
            "Component 33: 0.00%\n",
            "Component 34: 0.00%\n",
            "Component 35: 0.00%\n",
            "Component 36: 0.00%\n",
            "Component 37: 0.00%\n",
            "Component 38: 0.00%\n",
            "Component 39: 0.00%\n",
            "Component 40: 0.00%\n",
            "Component 41: 0.00%\n",
            "Component 42: 0.00%\n",
            "\n",
            "‚úÖ Keeping 12 components out of 42 (‚â•2% variance)\n",
            "\n",
            "üöÄ PCA completed! Reduced dataset saved as 'reduced_road_features_2percent.csv'.\n",
            "\n",
            "üîπ First 5 Rows of Reduced Feature Set:\n",
            "   PCA_Component_1  PCA_Component_2  PCA_Component_3  PCA_Component_4  \\\n",
            "0       141.435191       -25.296363        -0.558902         1.221387   \n",
            "1        -0.765480        -3.134948         0.326404         0.473550   \n",
            "2        -0.756155        -3.121219         0.371324         0.390367   \n",
            "3        -0.763832        -3.119476         0.295097         0.479366   \n",
            "4        -0.746120        -3.015059         0.347373         0.446473   \n",
            "\n",
            "   PCA_Component_5  PCA_Component_6  PCA_Component_7  PCA_Component_8  \\\n",
            "0         1.150972         1.703080        -0.646956        -2.540902   \n",
            "1         1.399338         0.068625        -0.246756        -0.238009   \n",
            "2         1.319204         0.238669         0.014431        -1.431420   \n",
            "3         0.619622         0.024541        -0.961342        -0.876526   \n",
            "4         0.861886         0.125059        -0.950068        -0.894697   \n",
            "\n",
            "   PCA_Component_9  PCA_Component_10  PCA_Component_11  PCA_Component_12  \n",
            "0        -0.990877          0.556236         -0.086859         -0.959560  \n",
            "1        -0.343819         -0.190109         -0.260450          0.147596  \n",
            "2         0.288824          0.448080         -1.437663          0.074247  \n",
            "3         0.869123         -1.041018          0.389564          0.529486  \n",
            "4         0.822203         -1.123193          0.366510          0.437533  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "timestamps missing from pca data\n",
        "\n",
        "The error occurs because the number of rows in sensor_with_gyro.csv does not match reduced_road_features_2percent.csv. This mismatch means we cannot directly reattach timestamps.\n",
        "\n",
        "The row count mismatch is small:\n",
        "\n",
        "PCA Features Dataset ‚Üí 15,372 rows\n",
        "Sensor with Gyro Dataset ‚Üí 15,381 rows (extra 9 rows)\n",
        "This suggests that some rows were dropped during PCA (likely due to missing values or outliers).\n",
        "\n",
        "classifying potholes using 3 models , svm , rf and rnn"
      ],
      "metadata": {
        "id": "cwByatxDwiLI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.spatial import KDTree\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import joblib\n",
        "\n",
        "# ‚úÖ Step 1: Load the Reduced Road Features Dataset\n",
        "feature_df = pd.read_csv(\"reduced_road_features_2percent.csv\")\n",
        "\n",
        "# ‚úÖ Step 2: Load Sensor Data to Retrieve Timestamps\n",
        "sensor_files = [\n",
        "    \"trip1_02-22-17_sensors.csv\",\n",
        "    \"trip2_02-22-17_sensors.csv\",\n",
        "    \"trip3_02-22-17_sensors.csv\",\n",
        "    \"03_26_trip1_sensors.csv\",\n",
        "    \"03_26_trip2_sensors.csv\",\n",
        "    \"04_02_trip1_sensors.csv\",\n",
        "]\n",
        "\n",
        "sensor_dfs = [pd.read_csv(file) for file in sensor_files]\n",
        "sensor_df = pd.concat(sensor_dfs, ignore_index=True)\n",
        "\n",
        "# Align timestamps by keeping only the first N rows to match PCA features\n",
        "feature_df[\"timestamp\"] = sensor_df[\"timestamp\"].iloc[:len(feature_df)].values\n",
        "\n",
        "# ‚úÖ Step 3: Load Pothole Data and Fix Timestamp Format\n",
        "pothole_files = [\n",
        "    \"trip1_02-22-17_potholes.csv\",\n",
        "    \"trip2_02-22-17_potholes.csv\",\n",
        "    \"trip3_02-22-17_potholes.csv\",\n",
        "]\n",
        "\n",
        "pothole_timestamps = set()\n",
        "for file in pothole_files:\n",
        "    pothole_data = pd.read_csv(file)\n",
        "    pothole_timestamps.update(pothole_data[\"timestamp\"].tolist())\n",
        "\n",
        "# Convert timestamps to numpy arrays for fast lookup\n",
        "feature_timestamps = feature_df[\"timestamp\"].values.reshape(-1, 1)\n",
        "pothole_timestamps_array = np.array(list(pothole_timestamps)).reshape(-1, 1)\n",
        "\n",
        "# ‚úÖ Step 4: Nearest Neighbor Matching for Pothole Labeling\n",
        "tree = KDTree(feature_timestamps)\n",
        "_, nearest_indices = tree.query(pothole_timestamps_array)\n",
        "\n",
        "# Mark the matched timestamps as potholes\n",
        "feature_df[\"pothole\"] = 0  # Default to No Pothole\n",
        "feature_df.loc[nearest_indices, \"pothole\"] = 1\n",
        "\n",
        "# ‚úÖ Step 5: Prepare Data for Training\n",
        "X = feature_df.drop(columns=[\"timestamp\", \"pothole\"])\n",
        "y = feature_df[\"pothole\"]\n",
        "\n",
        "# ‚úÖ Step 6: Split Data into Training (80%) and Testing (20%) Sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# ‚úÖ Step 7: Train SVM Model with Feature Scaling and Class Balancing\n",
        "svm_model = make_pipeline(StandardScaler(), SVC(kernel=\"rbf\", C=1.0, gamma=\"scale\", class_weight=\"balanced\"))\n",
        "\n",
        "# Perform Cross-Validation (5-Fold)\n",
        "cv_scores = cross_val_score(svm_model, X_train, y_train, cv=5, scoring=\"accuracy\")\n",
        "\n",
        "# Train the model on the full training set\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# ‚úÖ Step 8: Evaluate Model Performance\n",
        "test_accuracy = svm_model.score(X_test, y_test)\n",
        "\n",
        "# ‚úÖ Step 9: Save Trained Model\n",
        "joblib.dump(svm_model, \"svm_pothole_model.pkl\")\n",
        "\n",
        "# ‚úÖ Step 10: Display Results\n",
        "cv_results = pd.DataFrame({\"Cross-Validation Scores\": cv_scores})\n",
        "test_result = pd.DataFrame({\"Test Accuracy\": [test_accuracy]})\n",
        "\n",
        "# Print performance results\n",
        "print(\"\\nüöÄ SVM Pothole Detection Model Trained Successfully!\")\n",
        "print(f\"üîπ Test Accuracy: {test_accuracy:.2%}\")\n",
        "print(\"\\nüîπ Cross-Validation Scores:\")\n",
        "print(cv_results)\n",
        "\n",
        "# Save performance results as CSV\n",
        "cv_results.to_csv(\"svm_cv_results.csv\", index=False)\n",
        "test_result.to_csv(\"svm_test_accuracy.csv\", index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4R2Q6U2F9O5-",
        "outputId": "3a788420-1675-40a9-c5c3-3107c24fecb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üöÄ SVM Pothole Detection Model Trained Successfully!\n",
            "üîπ Test Accuracy: 89.66%\n",
            "\n",
            "üîπ Cross-Validation Scores:\n",
            "   Cross-Validation Scores\n",
            "0                 0.931707\n",
            "1                 0.915041\n",
            "2                 0.912566\n",
            "3                 0.908499\n",
            "4                 0.913379\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "rf\n"
      ],
      "metadata": {
        "id": "xmaESUr_7Lnw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.spatial import KDTree\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import joblib\n",
        "\n",
        "# ‚úÖ Step 1: Load the Reduced Road Features Dataset\n",
        "feature_df = pd.read_csv(\"reduced_road_features_2percent.csv\")\n",
        "\n",
        "# ‚úÖ Step 2: Load Sensor Data to Retrieve Timestamps\n",
        "sensor_files = [\n",
        "    \"trip1_02-22-17_sensors.csv\",\n",
        "    \"trip2_02-22-17_sensors.csv\",\n",
        "    \"trip3_02-22-17_sensors.csv\",\n",
        "    \"03_26_trip1_sensors.csv\",\n",
        "    \"03_26_trip2_sensors.csv\",\n",
        "    \"04_02_trip1_sensors.csv\",\n",
        "]\n",
        "\n",
        "sensor_dfs = [pd.read_csv(file) for file in sensor_files]\n",
        "sensor_df = pd.concat(sensor_dfs, ignore_index=True)\n",
        "\n",
        "# Align timestamps by keeping only the first N rows to match PCA features\n",
        "feature_df[\"timestamp\"] = sensor_df[\"timestamp\"].iloc[:len(feature_df)].values\n",
        "\n",
        "# ‚úÖ Step 3: Load Pothole Data and Fix Timestamp Format\n",
        "pothole_files = [\n",
        "    \"trip1_02-22-17_potholes.csv\",\n",
        "    \"trip2_02-22-17_potholes.csv\",\n",
        "    \"trip3_02-22-17_potholes.csv\",\n",
        "]\n",
        "\n",
        "pothole_timestamps = set()\n",
        "for file in pothole_files:\n",
        "    pothole_data = pd.read_csv(file)\n",
        "    pothole_timestamps.update(pothole_data[\"timestamp\"].tolist())\n",
        "\n",
        "# Convert timestamps to numpy arrays for fast lookup\n",
        "feature_timestamps = feature_df[\"timestamp\"].values.reshape(-1, 1)\n",
        "pothole_timestamps_array = np.array(list(pothole_timestamps)).reshape(-1, 1)\n",
        "\n",
        "# ‚úÖ Step 4: Nearest Neighbor Matching for Pothole Labeling\n",
        "tree = KDTree(feature_timestamps)\n",
        "_, nearest_indices = tree.query(pothole_timestamps_array)\n",
        "\n",
        "# Mark the matched timestamps as potholes\n",
        "feature_df[\"pothole\"] = 0  # Default to No Pothole\n",
        "feature_df.loc[nearest_indices, \"pothole\"] = 1\n",
        "\n",
        "# ‚úÖ Step 5: Prepare Data for Training\n",
        "X = feature_df.drop(columns=[\"timestamp\", \"pothole\"])\n",
        "y = feature_df[\"pothole\"]\n",
        "\n",
        "# ‚úÖ Step 6: Split Data into Training (80%) and Testing (20%) Sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# ‚úÖ Step 7: Train Random Forest Model with Feature Scaling and Class Balancing\n",
        "rf_model = make_pipeline(StandardScaler(), RandomForestClassifier(n_estimators=100, random_state=42, class_weight=\"balanced\"))\n",
        "\n",
        "# Perform Cross-Validation (5-Fold)\n",
        "rf_cv_scores = cross_val_score(rf_model, X_train, y_train, cv=5, scoring=\"accuracy\")\n",
        "\n",
        "# Train the model on the full training set\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# ‚úÖ Step 8: Evaluate Model Performance\n",
        "rf_test_accuracy = rf_model.score(X_test, y_test)\n",
        "\n",
        "# ‚úÖ Step 9: Save Trained Model\n",
        "joblib.dump(rf_model, \"rf_pothole_model.pkl\")\n",
        "\n",
        "# ‚úÖ Step 10: Display Results\n",
        "rf_cv_results = pd.DataFrame({\"Cross-Validation Scores\": rf_cv_scores})\n",
        "rf_test_result = pd.DataFrame({\"Test Accuracy\": [rf_test_accuracy]})\n",
        "\n",
        "# Print performance results\n",
        "print(\"\\nüöÄ Random Forest Pothole Detection Model Trained Successfully!\")\n",
        "print(f\"üîπ Test Accuracy: {rf_test_accuracy:.2%}\")\n",
        "print(\"\\nüîπ Cross-Validation Scores:\")\n",
        "print(rf_cv_results)\n",
        "\n",
        "# Save performance results as CSV\n",
        "rf_cv_results.to_csv(\"rf_cv_results.csv\", index=False)\n",
        "rf_test_result.to_csv(\"rf_test_accuracy.csv\", index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fivx8Bfc7NOB",
        "outputId": "7fdf40d8-6014-49e2-f425-546412a7c6ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üöÄ Random Forest Pothole Detection Model Trained Successfully!\n",
            "üîπ Test Accuracy: 99.64%\n",
            "\n",
            "üîπ Cross-Validation Scores:\n",
            "   Cross-Validation Scores\n",
            "0                 0.996341\n",
            "1                 0.996341\n",
            "2                 0.996747\n",
            "3                 0.996747\n",
            "4                 0.996340\n"
          ]
        }
      ]
    }
  ]
}